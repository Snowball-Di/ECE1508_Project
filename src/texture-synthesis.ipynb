{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, models\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "MAX_SIZE = 400\n",
    "\n",
    "def load_image(img_path, max_size=MAX_SIZE, shape=None):\n",
    "\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    size = min(max_size, max(image.size))\n",
    "\n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "\n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def im_convert(tensor):\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + \\\n",
    "        np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, use_leaky_relu=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2, inplace=True) if use_leaky_relu else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class JoinBlock(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        # Resize x1 to the size of x2 for concatenation\n",
    "        upsampled_x1 = F.interpolate(x1, size=x2.shape[2:], mode='nearest')\n",
    "        return torch.cat((upsampled_x1, x2), dim=1)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Define the convolutional blocks for each level\n",
    "        self.conv_block_1 = self._make_layers(3, 8)\n",
    "        self.conv_block_2 = self._make_layers(3, 8)\n",
    "        self.conv_block_3 = self._make_layers(3, 8)\n",
    "        self.conv_block_4 = self._make_layers(3, 8)\n",
    "        self.conv_block_5 = self._make_layers(3, 8)\n",
    "\n",
    "        # Define the join blocks\n",
    "        self.join_block_1 = JoinBlock()\n",
    "        self.join_block_2 = JoinBlock()\n",
    "        self.join_block_3 = JoinBlock()\n",
    "        self.join_block_4 = JoinBlock()\n",
    "\n",
    "        # Define the post-join convolutional blocks\n",
    "        self.post_join_conv_block_1 = self._make_layers(16, 16)\n",
    "        self.post_join_conv_block_2 = self._make_layers(24, 24)\n",
    "        self.post_join_conv_block_3 = self._make_layers(32, 32)\n",
    "        self.post_join_conv_block_4 = self._make_layers(40, 40)\n",
    "\n",
    "        # Final convolutional layer to create the output texture image\n",
    "        self.final_conv = ConvBlock(40, 3, 1, use_leaky_relu=False)\n",
    "\n",
    "    def _make_layers(self, in_channels, out_channels):\n",
    "        layers = [\n",
    "            ConvBlock(in_channels, out_channels, 3),\n",
    "            ConvBlock(out_channels, out_channels, 3),\n",
    "            ConvBlock(out_channels, out_channels, 1)\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process the input through each convolutional block at each scale\n",
    "        conv1 = self.conv_block_1(x)\n",
    "        conv2 = self.conv_block_2(F.interpolate(x, scale_factor=1/2, mode='bilinear', align_corners=True))\n",
    "        conv3 = self.conv_block_3(F.interpolate(x, scale_factor=1/4, mode='bilinear', align_corners=True))\n",
    "        conv4 = self.conv_block_4(F.interpolate(x, scale_factor=1/8, mode='bilinear', align_corners=True))\n",
    "        conv5 = self.conv_block_5(F.interpolate(x, scale_factor=1/16, mode='bilinear', align_corners=True))\n",
    "\n",
    "        # Perform the joining operations\n",
    "        join1 = self.join_block_1(conv5, conv4)\n",
    "        join2 = self.join_block_2(self.post_join_conv_block_1(join1), conv3)\n",
    "        join3 = self.join_block_3(self.post_join_conv_block_2(join2), conv2)\n",
    "        join4 = self.join_block_4(self.post_join_conv_block_3(join3), conv1)\n",
    "\n",
    "        # Final processing after the last join operation\n",
    "        result = self.post_join_conv_block_4(join4)\n",
    "\n",
    "        # Create the final output texture image\n",
    "        texture = self.final_conv(result)\n",
    "        return texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.nn import functional as F\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def get_features(image, model, target_layers=['0', '5', '10', '19', '21', '28']):\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in target_layers:\n",
    "            features[name] = x\n",
    "    return features\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    _, C, H, W = tensor.size()\n",
    "    tensor = tensor.view(C, H * W)\n",
    "    # Compute the Gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "\n",
    "    return gram\n",
    "\n",
    "# Assume generator is your pre-defined Generator class\n",
    "generator = Generator().to(device)\n",
    "# load in content and style image\n",
    "content = load_image('images/octopus.jpg').to(device)\n",
    "# Resize style to match content, makes code easier\n",
    "style = load_image('images/hockney.jpg', shape=content.shape[-2:]).to(device)\n",
    "vgg = models.vgg19().features\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "vgg.to(device)\n",
    "\n",
    "content_features = get_features(content, vgg)\n",
    "style_features = get_features(style, vgg)\n",
    "style_grams = {name:gram_matrix(style_features[name]) for name in style_features}\n",
    "\n",
    "target = content.clone().requires_grad_(True).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "# After training, the generator should be able to generate images in the style of the style image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weights = {'0':1, '5':0.75, '10':0.2, '19':0.2, '21':0.2, '28':0.2}\n",
    "content_weight = 1  # alpha\n",
    "style_weight = 1e6  # beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 146/2000 [00:25<05:27,  5.65it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 1.20 GB, other allocations: 25.95 GB, max allowed: 27.20 GB). Tried to allocate 57.81 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m content_weight \u001b[38;5;241m*\u001b[39m content_loss \u001b[38;5;241m+\u001b[39m style_weight \u001b[38;5;241m*\u001b[39m style_loss\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m  ii \u001b[38;5;241m%\u001b[39m show_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 1.20 GB, other allocations: 25.95 GB, max allowed: 27.20 GB). Tried to allocate 57.81 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# for displaying the target image, intermittently\n",
    "import tqdm\n",
    "\n",
    "show_every = 400\n",
    "\n",
    "# iteration hyperparameters\n",
    "optimizer = optim.Adam([target], lr=0.003)\n",
    "steps = 2000  # decide how many iterations to update your image (5000)\n",
    "\n",
    "for ii in tqdm.tqdm(range(1, steps+1)):\n",
    "    \n",
    "    target_features = get_features(target, vgg)\n",
    "    content_loss = torch.mean((target_features['21'] - content_features['21'])**2)\n",
    "    \n",
    "    style_loss = 0\n",
    "    \n",
    "    # iterate through each style layer and add to the style loss\n",
    "    for layer in style_weights:\n",
    "        # get the \"target\" style representation for the layer\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        _, d, h, w = target_feature.shape\n",
    "        \n",
    "        style_gram = style_grams[layer]\n",
    "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "        \n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if  ii % show_every == 0:\n",
    "        print('Total loss: ', total_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = load_image('images/rainier.jpg').to(device)\n",
    "with torch.no_grad():\n",
    "    stylized_image = generator(content_image)\n",
    "# display content and final, target image\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(40, 20))\n",
    "ax1.imshow(im_convert(content_image))\n",
    "ax2.imshow(im_convert(stylized_image))\n",
    "ax3.imshow(im_convert(stylized_image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
